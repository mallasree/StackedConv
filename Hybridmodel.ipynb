{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": "",
  "signature": "sha256:ef7e3c7bd9a81014c419880b90d861e363f36b9a8db7d473158005c9cd1c0d28"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "#from sklearn.cross_validation import train_test_split\n",
      "text1P=pd.read_table(\"/home/sreenu/Desktop/dataset4may/Paperswithcode/applyandcompareneuralmodels/data/nepal/Availabilitynepal.txt\",header=None,names=['tweet1'],encoding='utf-8')\n",
      "text1P['label']=1\n",
      "#print text1P.shape\n",
      "#print len(text1P)\n",
      "text1P1=pd.read_table(\"/home/sreenu/Desktop/dataset4may/Paperswithcode/applyandcompareneuralmodels/data/nepal/neednepal.txt\",header=None,names=['tweet1'],encoding='utf-8')\n",
      "text1P1['label']=2\n",
      "print text1P1.shape\n",
      "text1N=pd.read_table(\"/home/sreenu/Desktop/dataset4may/FIRE/FIRE2016-microblogs-track-data/Nonrelevantresource.txt\",header=None,names=['tweet1'],encoding='utf-8')\n",
      "text1N['label']=0\n",
      "textTrainP=text1P[:372]\n",
      "textTestP=text1P[372:464]\n",
      "textTrainP1=text1P1[:372]\n",
      "textTestP1=text1P1[372:]\n",
      "textTrainN=text1N[:372]\n",
      "textTestN=text1N[372:464]\n",
      "framesTrain=[textTrainP,textTrainP1,textTrainN]\n",
      "framesTest=[textTestP,textTestP1,textTestN]\n",
      "train=pd.concat(framesTrain)\n",
      "test=pd.concat(framesTest)\n",
      "X_train=train['tweet1']\n",
      "y_train=train['label']\n",
      "X_test=test['tweet1']\n",
      "y_test=test['label']\n",
      "print textTestP.shape\n",
      "print X_train.shape\n",
      "print X_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Italy Earthquake equal size need and availability 80 training and 20 testing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.cross_validation import train_test_split\n",
      "text1P=pd.read_table(\"/home/sreenu/Desktop/dataset4may/Paperswithcode/applyandcompareneuralmodels/data/italy/availitaly.txt\",header=None,names=['tweet1'],encoding='utf-8')\n",
      "text1P['label']=1\n",
      "print text1P.shape\n",
      "#print len(text1P)\n",
      "text1P1=pd.read_table(\"/home/sreenu/Desktop/dataset4may/Paperswithcode/applyandcompareneuralmodels/data/italy/needitaly.txt\",header=None,names=['tweet1'],encoding='utf-8')\n",
      "text1P1['label']=2\n",
      "print text1P1.shape\n",
      "text1N=pd.read_table(\"/home/sreenu/Desktop/dataset4may/Paperswithcode/applyandcompareneuralmodels/data/italy/nonrelevantresource.txt\",header=None,names=['tweet1'],encoding='utf-8')\n",
      "text1N['label']=0\n",
      "textTrainP=text1P[:120]\n",
      "textTestP=text1P[120:149]\n",
      "textTrainP1=text1P1[:120]\n",
      "textTestP1=text1P1[120:]\n",
      "textTrainN=text1N[:120]\n",
      "textTestN=text1N[120:149]\n",
      "framesTrain=[textTrainP,textTrainP1,textTrainN]\n",
      "framesTest=[textTestP,textTestP1,textTestN]\n",
      "train=pd.concat(framesTrain)\n",
      "test=pd.concat(framesTest)\n",
      "X_train=train['tweet1']\n",
      "y_train=train['label']\n",
      "X_test=test['tweet1']\n",
      "y_test=test['label']\n",
      "print textTrainP.shape\n",
      "print X_train.shape\n",
      "print X_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.model_selection import KFold\n",
      "import pandas as pd\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras.preprocessing.sequence import pad_sequences\n",
      "from keras.utils import to_categorical\n",
      "import numpy as np\n",
      "#from sklearn.cross_validation import train_test_split\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from nltk.corpus import stopwords\n",
      "from sklearn import svm\n",
      "import gensim\n",
      "from gensim.models import Word2Vec\n",
      "from gensim.utils import simple_preprocess\n",
      "from gensim.models.keyedvectors import KeyedVectors\n",
      "#text1=pd.read_table(\"/home/sreenu/Desktop/dataset4may/SMERP/RetrievedSMERP/level-1/TotalSMERP_T31.tsv\",header=None,names=['tweet1','label','label1'],encoding='utf-8')\n",
      "#text2=text1.dropna(thresh=0.8*len(text1), axis=1) # reading dataset link\n",
      "textTrainP=text2[:878]\n",
      "textTestP=text2[878:1170]\n",
      "textTrainN=text2[1170:2048]\n",
      "textTestN=text2[2048:]\n",
      "framesTrain=[textTrainP,textTrainN]\n",
      "framesTest=[textTestP,textTestN]\n",
      "X_train=pd.concat(framesTrain)['tweet1']\n",
      "y_train=pd.concat(framesTrain)['label']\n",
      "X_test=pd.concat(framesTest)['tweet1']\n",
      "y_test=pd.concat(framesTest)['label']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "texts=train['tweet1']\n",
      "NUM_WORDS=20000\n",
      "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
      "                      lower=True)\n",
      "tokenizer.fit_on_texts(texts)\n",
      "sequences_train = tokenizer.texts_to_sequences(texts)\n",
      "sequences_valid=tokenizer.texts_to_sequences(test['tweet1'])\n",
      "word_index = tokenizer.word_index\n",
      "X_train1 = pad_sequences(sequences_train)\n",
      "X_val = pad_sequences(sequences_valid,maxlen=X_train1.shape[1])\n",
      "y_train1 = to_categorical(train.label)\n",
      "y_val = to_categorical(test['label'])\n",
      "#print('Found %s unique tokens.' % word_index)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Reading the Pre-trained Crisis word embeddings\n",
      "\n",
      "word_vectors = KeyedVectors.load_word2vec_format('crisisNLP_word_vector.bin', binary=True)\n",
      "EMBEDDING_DIM=300\n",
      "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
      "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
      "for word, i in word_index.items():\n",
      "    if i>=NUM_WORDS:\n",
      "        continue\n",
      "    try:\n",
      "        embedding_vector = word_vectors[word]\n",
      "        embedding_matrix[i] = embedding_vector\n",
      "    except KeyError:\n",
      "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
      "\n",
      "del(word_vectors)\n",
      "\n",
      "from keras.layers import Embedding\n",
      "embedding_layer = Embedding(vocabulary_size,\n",
      "                            EMBEDDING_DIM,\n",
      "                            weights=[embedding_matrix],\n",
      "                            trainable=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
      "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
      "from keras.models import Model\n",
      "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
      "from keras.layers.core import Reshape, Flatten\n",
      "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
      "from keras.optimizers import Adam\n",
      "from keras import regularizers\n",
      "sequence_length = X_train1.shape[1]\n",
      "filter_sizes = [3,4,5]\n",
      "num_filters = 100\n",
      "drop = 0.5\n",
      "inputs = Input(shape=(sequence_length,))\n",
      "embedding = embedding_layer(inputs)\n",
      "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
      "\n",
      "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
      "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
      "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
      "\n",
      "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
      "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
      "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
      "\n",
      "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
      "flatten = Flatten()(merged_tensor)\n",
      "reshape = Reshape((3*num_filters,))(flatten)\n",
      "dropout = Dropout(drop)(flatten)\n",
      "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
      "\n",
      "# this creates a model that includes\n",
      "model = Model(inputs, output)\n",
      "adam = Adam(lr=1e-3)\n",
      "\n",
      "model.compile(loss='categorical_crossentropy',\n",
      "              optimizer=adam,\n",
      "              metrics=['accuracy'])\n",
      "#filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
      "#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
      "#callbacks = [checkpoint]\n",
      "callbacks = [EarlyStopping(monitor='val_acc')]\n",
      "#print y_val\n",
      "#for i in range(70):\n",
      "history=model.fit(X_train1, y_train1, validation_split=0.1,batch_size=128, epochs=25, verbose=1, callbacks=callbacks) \n",
      "#history=model.fit(X_train1, y_train1, batch_size=1000, epochs=10, verbose=1,callbacks=callbacks) \n",
      "\n",
      "#model.fit(X_train, y_train, batch_size=1000, epochs=10, verbose=1)\n",
      "scores = model.evaluate(X_val, y_val, verbose=0)\n",
      "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from keras.models import model_from_json\n",
      "# serialize model to JSON\n",
      "model_json = model.to_json()\n",
      "with open(\"modelneptotal.json\", \"w\") as json_file:\n",
      "    json_file.write(model_json)\n",
      "# serialize weights to HDF5\n",
      "model.save_weights(\"modelneptotal.h5\")\n",
      "print(\"Saved model to disk\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from keras.models import model_from_json\n",
      "from keras.optimizers import Adam \n",
      "# later...\n",
      " \n",
      "# load json and create model\n",
      "json_file = open('modelnepal2.json', 'r')\n",
      "loaded_model_json = json_file.read()\n",
      "json_file.close()\n",
      "loaded_model = model_from_json(loaded_model_json)\n",
      "# load weights into new model\n",
      "loaded_model.load_weights(\"modelnepal2.h5\")\n",
      "print(\"Loaded model from disk\")\n",
      "adam = Adam(lr=1e-3)\n",
      "# evaluate loaded model on test data\n",
      "loaded_model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
      "score = loaded_model.evaluate(X_val, y_val, verbose=0)\n",
      "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print (X_val.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from keras.callbacks import EarlyStopping,History\n",
      "callbacks = [EarlyStopping(monitor='val_loss'),History()]\n",
      "#loaded_model.fit(X_train1, y_train1, batch_size=1000, epochs=10, verbose=1,callbacks=callbacks) \n",
      "#loaded_model.fit(X_train1, y_train1, batch_size=1000, epochs=2, verbose=1)\n",
      "#print model.summary()\n",
      "#print model.nb_epoch\n",
      "scores =loaded_model.evaluate(X_val, y_val, verbose=0)\n",
      "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], scores[1]*100))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "y_pred=loaded_model.predict(X_val)\n",
      "y_pred_labels = np.argmax(y_pred, axis=1)  # only necessary if output has one-hot-encoding, shape=(n_samples)\n",
      "confusion_matrix = metrics.confusion_matrix(y_true=test.label, y_pred=y_pred_labels)\n",
      "print metrics.classification_report(test.label, y_pred_labels)\n",
      "print metrics.accuracy_score(test.label,y_pred_labels)\n",
      "print confusion_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import string \n",
      "import json \n",
      "from collections import Counter \n",
      "from nltk.tokenize import TweetTokenizer \n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "stemmer = SnowballStemmer(\"english\")\n",
      "wnl = WordNetLemmatizer()\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import re\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn import metrics\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.svm import SVC\n",
      "from sklearn import svm\n",
      "import numpy as np\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.feature_extraction import text \n",
      "\n",
      "def process(text, tokenizer=TweetTokenizer(), stopwords=[]): \n",
      "  \"\"\"Process the text of a tweet: \n",
      "  - Lowercase \n",
      "  - Tokenize \n",
      "  - Stopword removal \n",
      "  - Digits removal \n",
      " \n",
      "  Return: list of strings \n",
      "  \"\"\" \n",
      "  \n",
      "  text = text.lower()\n",
      "  tokens = tokenizer.tokenize(text)\n",
      "  return [tok for tok in tokens if tok not in stopwords and not any(i.isdigit() for i in tok)  and len(tok)>2 and tok is tok.strip('#') and tok is tok.strip('@')  and tok not in (tok for tok in tokens if re.search('http', tok)) ]\n",
      "my_additional_stop_words=['amp','https','ud83d','nhttps','rieti','u2026','rt','http','37','10','38','di','24','36','...','the']\n",
      "stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)\n",
      "#vocabulary1IG=['timesnow','special','breaking','donating','earth','minister','centre','immediate','today','doing','affected','operation','personnel','great','airport','asked','damaged','workers','following','power']\n",
      "vocabularych=['aid','blood','daily','delhi','food','meals','medical','need','needed','needs','packets','people','send','shortage','sikh','temple','tents','thousands','urgently','water']\n",
      "vocabularymuta=['awesome','contacted','everyday','flight','food','forces','free','google','gorkha','granted','need','organising','send','sheeting','targeting','team','technicians','temple','treated','warm']\n",
      "CHI=['aid','daily','delhi','earthquake','food','help','meals','medical','need','needed','needs','packets','send','shelter','temple','tents','urgent','urgently','victims','water']\n",
      "vectorizer=CountVectorizer(tokenizer=process,min_df=1,ngram_range=(1,1),stop_words=stop_words,vocabulary=CHI)\n",
      "#vectorizer1=CountVectorizer(tokenizer=process,min_df=1,ngram_range=(1,1),stop_words=stop_words,vocabulary=CHI)\n",
      "#X2=vectorizer1.fit_transform(X_train)\n",
      "X1 = vectorizer.fit_transform(X_train)\n",
      "#print vectorizer.get_feature_names()\n",
      "X1 = X1.toarray()\n",
      "#X2=X2.toarray()\n",
      "X_test1 = vectorizer.transform(X_test)\n",
      "#X_test2=vectorizer1.transform(X_test)\n",
      "X_test1=X_test1.toarray()\n",
      "#X_test2=X_test2.toarray()\n",
      "#from sklearn.naive_bayes import MultinomialNB\n",
      "#clf2=MultinomialNB()\n",
      "#from sklearn.neighbors import KNeighborsClassifier\n",
      "#clf2 = KNeighborsClassifier(n_neighbors=5)\n",
      "#clf1 =SVC(kernel=\"rbf\",probability=True)\n",
      "clf2=SVC(kernel=\"rbf\")\n",
      "#from sklearn.tree import DecisionTreeClassifier\n",
      "#clf2 = DecisionTreeClassifier(random_state=0)\n",
      "#%time clf1.fit(X1, y_train)\n",
      "clf2.fit(X1, y_train)\n",
      "#y_pred_class=clf1.predict(X1)\n",
      "#y_pred_class=clf1.predict_proba(X1)\n",
      "y_pred_class1=clf2.predict(X1)\n",
      "\n",
      "#clf3 =SVC(kernel=\"rbf\",probability=True)\n",
      "#clf4=SVC(kernel=\"rbf\")\n",
      "#%time clf3.fit(X2, y_train)\n",
      "#clf4.fit(X2, y_train)\n",
      "#y_pred_class=clf1.predict(X1)\n",
      "#y_pred_class2=clf3.predict_proba(X2)\n",
      "#y_pred_class3=clf4.predict(X2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sequences_test=tokenizer.texts_to_sequences(test['tweet1'])\n",
      "#X_test = pad_sequences(sequences_test,maxlen=X_train1.shape[1])\n",
      "#y_test = y_val\n",
      "#y_pred=[]\n",
      "#r=X_test\n",
      "y_pred=loaded_model.predict(X_train1)\n",
      "#for i in X_test:\n",
      "   # y_pred.append(model.predict(i))\n",
      "#np.concatenate((y_pred, y_pred_class.T), axis=1)\n",
      "#y_pred=np.array(y_pred)\n",
      "print y_pred.shape\n",
      "y_pred_class = np.array(y_pred_class1)\n",
      "y_pred_class = y_pred_class[:, np.newaxis]\n",
      "print y_pred_class.shape\n",
      "X1_new=np.hstack((y_pred, y_pred_class))\n",
      "#clf1_new =SVC(kernel=\"rbf\")\n",
      "#from sklearn.naive_bayes import MultinomialNB\n",
      "#clf1_new=MultinomialNB()\n",
      "#from sklearn.tree import DecisionTreeClassifier\n",
      "#clf1_new = DecisionTreeClassifier(random_state=0)\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "clf1_new = KNeighborsClassifier(n_neighbors=5)\n",
      "print X1_new.shape\n",
      "clf1_new.fit(X1_new, y_train)\n",
      "#clf1_new.fit(y_pred, y_train)\n",
      "#y_pred_class_new=clf1_new.predict(X_test1)\n",
      "#print metrics.classification_report(y_test, y_pred_class)\n",
      "#from sklearn import metrics\n",
      "#print metrics.accuracy_score(y_test,y_pred_class)\n",
      "#print confusion_matrix(y_test,y_pred_class)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Naive bayesian classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "clf1_new=MultinomialNB()\n",
      "%time clf1_new.fit(y_pred, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Decision Tree"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.tree import DecisionTreeClassifier\n",
      "clf1_new = DecisionTreeClassifier(random_state=0)\n",
      "clf1_new.fit(y_pred, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Testing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sequences_test=tokenizer.texts_to_sequences(test['tweet1'])\n",
      "X_test = pad_sequences(sequences_test,maxlen=X_train1.shape[1])\n",
      "#print X_test.shape\n",
      "y_test = y_val\n",
      "y_pred_t=loaded_model.predict(X_test)\n",
      "#y_pred_t=np.array(y_pred_t)\n",
      "y_pred_class_t=clf2.predict(X_test1)\n",
      "#y_pred_class_t1=clf3.predict_proba(X_test2)\n",
      "\n",
      "y_pred_class_t = np.array(y_pred_class_t)\n",
      "y_pred_class_t = y_pred_class_t[:, np.newaxis]\n",
      "print y_pred_class_t.shape\n",
      "print y_pred_t.shape\n",
      "X1_new_t=np.hstack((y_pred_t, y_pred_class_t))\n",
      "#print X1_new_t.shape\n",
      "y_pred_class_new=clf1_new.predict(X1_new_t)\n",
      "#y_pred_class_new=clf1_new.predict(y_pred_t)\n",
      "#print test.label.shape\n",
      "#print y_pred_class_new.shape\n",
      "print metrics.classification_report(test.label, y_pred_class_new)\n",
      "#from sklearn import metrics\n",
      "print metrics.accuracy_score(test.label,y_pred_class_new)\n",
      "confusion_matrix = metrics.confusion_matrix(y_true=test.label, y_pred=y_pred_class_new)\n",
      "print confusion_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}